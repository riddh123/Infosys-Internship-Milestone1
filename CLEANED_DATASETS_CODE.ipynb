{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ETH"
      ],
      "metadata": {
        "id": "U3zWGEOf3PkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_eth=pd.read_csv(\"content/ETHUSD/T_1h_2017_2025.csv\")"
      ],
      "metadata": {
        "id": "M-RhVEnb3S4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1. Imports\n",
        "# -------------------------\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# -------------------------\n",
        "# 2. Load ETH dataset\n",
        "# -------------------------\n",
        "df = pd.read_csv('/content/ETHUSDT_1h_2017_2025.csv')  # Replace with your ETH CSV path\n",
        "\n",
        "# Convert timestamp columns automatically\n",
        "df['Open Time'] = pd.to_datetime(df['Open Time'], errors='coerce')\n",
        "df['Close Time'] = pd.to_datetime(df['Close Time'], errors='coerce')\n",
        "\n",
        "# Drop rows where timestamp conversion failed\n",
        "df.dropna(subset=['Open Time','Close Time'], inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 3. Keep only relevant columns\n",
        "# -------------------------\n",
        "df = df[['Open Time','Open','High','Low','Close','Volume','Quote Volume','Trades']]\n",
        "\n",
        "# -------------------------\n",
        "# 4. Handle missing values\n",
        "# -------------------------\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 5. Feature Engineering\n",
        "# -------------------------\n",
        "df['HL_PCT'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
        "df['PCT_change'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
        "\n",
        "# -------------------------\n",
        "# 6. Scaling / Normalization\n",
        "# -------------------------\n",
        "features = ['Open','High','Low','Close','Volume','Quote Volume','Trades','HL_PCT','PCT_change']\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
        "scaled_df['Open Time'] = df['Open Time'].values\n",
        "\n",
        "# -------------------------\n",
        "# 7. Save entire cleaned & scaled dataset\n",
        "# -------------------------\n",
        "scaled_df.to_csv('ETH_cleaned_scaled.csv', index=False)\n",
        "print(\"Entire ETH cleaned & scaled dataset saved successfully as 'ETH_cleaned_scaled.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuEL3tLWGaki",
        "outputId": "8b3e6ee3-7eca-43cd-a088-62d750f439a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entire ETH cleaned & scaled dataset saved successfully as 'ETH_cleaned_scaled.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XRP"
      ],
      "metadata": {
        "id": "z3NcNnLyCWCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv('/content/XRPUSDT_1h_2017_2025.csv')\n",
        "\n",
        "# Check column names\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3zdeHnHCGUq",
        "outputId": "e1d868cb-3645-4340-bbc2-af68ac6905d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close Time',\n",
            "       'Quote Volume', 'Trades', 'Taker Buy Base', 'Taker Buy Quote',\n",
            "       'Ignore'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1. Imports\n",
        "# -------------------------\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# -------------------------\n",
        "# 2. Load XRP dataset\n",
        "# -------------------------\n",
        "df = pd.read_csv('/content/XRPUSDT_1h_2017_2025.csv')\n",
        "\n",
        "# Convert timestamp columns\n",
        "df['Open Time'] = pd.to_datetime(df['Open Time'])\n",
        "df['Close Time'] = pd.to_datetime(df['Close Time'])\n",
        "\n",
        "# -------------------------\n",
        "# 3. Drop unwanted columns\n",
        "# -------------------------\n",
        "df = df[['Open Time','Open','High','Low','Close','Volume','Quote Volume','Trades']]\n",
        "\n",
        "# -------------------------\n",
        "# 4. Handle missing values\n",
        "# -------------------------\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 5. Feature Engineering\n",
        "# -------------------------\n",
        "df['HL_PCT'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
        "df['PCT_change'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
        "\n",
        "# Select features for scaling\n",
        "features = ['Open','High','Low','Close','Volume','Quote Volume','Trades','HL_PCT','PCT_change']\n",
        "processed_df = df[['Open Time'] + features].copy()\n",
        "\n",
        "# -------------------------\n",
        "# 6. Scaling / Normalization\n",
        "# -------------------------\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(processed_df[features])\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
        "scaled_df['Open Time'] = processed_df['Open Time'].values\n",
        "\n",
        "# -------------------------\n",
        "# 7. Train-Test Split (time series style)\n",
        "# -------------------------\n",
        "train_size = int(len(scaled_df) * 0.8)\n",
        "train_df = scaled_df[:train_size]\n",
        "test_df = scaled_df[train_size:]\n",
        "\n",
        "# -------------------------\n",
        "# 8. Save processed files in Colab\n",
        "# -------------------------\n",
        "scaled_df.to_csv('XRP_cleaned_scaled.csv', index=False)\n",
        "\n",
        "print(\"Entire cleaned & scaled dataset saved successfully as 'XRP_cleaned_scaled.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "056zKpTCCYaQ",
        "outputId": "50c140c7-4aa5-4e39-e62e-7ccefdd8223d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entire cleaned & scaled dataset saved successfully as 'XRP_cleaned_scaled.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LTC"
      ],
      "metadata": {
        "id": "UsUkUkeJDAEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1. Imports\n",
        "# -------------------------\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# -------------------------\n",
        "# 2. Load LTC dataset\n",
        "# -------------------------\n",
        "df = pd.read_csv('/content/LTCUSDT_1h_2017_2025.csv')\n",
        "\n",
        "# Convert timestamp columns automatically\n",
        "df['Open Time'] = pd.to_datetime(df['Open Time'], errors='coerce')\n",
        "df['Close Time'] = pd.to_datetime(df['Close Time'], errors='coerce')\n",
        "\n",
        "# Drop any rows where timestamp conversion failed\n",
        "df.dropna(subset=['Open Time','Close Time'], inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 3. Drop unwanted columns\n",
        "# -------------------------\n",
        "df = df[['Open Time','Open','High','Low','Close','Volume','Quote Volume','Trades']]\n",
        "\n",
        "# -------------------------\n",
        "# 4. Handle missing values\n",
        "# -------------------------\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 5. Feature Engineering\n",
        "# -------------------------\n",
        "df['HL_PCT'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
        "df['PCT_change'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
        "\n",
        "# -------------------------\n",
        "# 6. Scaling / Normalization\n",
        "# -------------------------\n",
        "features = ['Open','High','Low','Close','Volume','Quote Volume','Trades','HL_PCT','PCT_change']\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
        "scaled_df['Open Time'] = df['Open Time'].values\n",
        "\n",
        "# -------------------------\n",
        "# 7. Save entire cleaned & scaled dataset\n",
        "# -------------------------\n",
        "scaled_df.to_csv('LTC_cleaned_scaled.csv', index=False)\n",
        "print(\"Entire LTC cleaned & scaled dataset saved successfully as 'LTC_cleaned_scaled.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjb43YqoDj1W",
        "outputId": "c4b5c3cf-cd13-4964-ddf8-f4083e2b11e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entire LTC cleaned & scaled dataset saved successfully as 'LTC_cleaned_scaled.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BNB"
      ],
      "metadata": {
        "id": "PaScPLe4D2hF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1. Imports\n",
        "# -------------------------\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# -------------------------\n",
        "# 2. Load BNB dataset\n",
        "# -------------------------\n",
        "df = pd.read_csv('/content/BNBUSDT_1h_2017_2025.csv')  # Replace with your file path\n",
        "\n",
        "# Convert timestamp columns automatically\n",
        "df['Open Time'] = pd.to_datetime(df['Open Time'], errors='coerce')\n",
        "df['Close Time'] = pd.to_datetime(df['Close Time'], errors='coerce')\n",
        "\n",
        "# Drop rows where timestamp conversion failed\n",
        "df.dropna(subset=['Open Time','Close Time'], inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 3. Keep only relevant columns\n",
        "# -------------------------\n",
        "df = df[['Open Time','Open','High','Low','Close','Volume','Quote Volume','Trades']]\n",
        "\n",
        "# -------------------------\n",
        "# 4. Handle missing values\n",
        "# -------------------------\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 5. Feature Engineering\n",
        "# -------------------------\n",
        "df['HL_PCT'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
        "df['PCT_change'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
        "\n",
        "# -------------------------\n",
        "# 6. Scaling / Normalization\n",
        "# -------------------------\n",
        "features = ['Open','High','Low','Close','Volume','Quote Volume','Trades','HL_PCT','PCT_change']\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
        "scaled_df['Open Time'] = df['Open Time'].values\n",
        "\n",
        "# -------------------------\n",
        "# 7. Save entire cleaned & scaled dataset\n",
        "# -------------------------\n",
        "scaled_df.to_csv('BNB_cleaned_scaled.csv', index=False)\n",
        "print(\"Entire BNB cleaned & scaled dataset saved successfully as 'BNB_cleaned_scaled.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skJeOujRD3wl",
        "outputId": "218d1b83-2cae-4051-a8b6-22e858da4595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entire BNB cleaned & scaled dataset saved successfully as 'BNB_cleaned_scaled.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BTC"
      ],
      "metadata": {
        "id": "ME5aYMNhGJnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1. Imports\n",
        "# -------------------------\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# -------------------------\n",
        "# 2. Load BTC dataset\n",
        "# -------------------------\n",
        "df = pd.read_csv('/content/BTCUSDT_1h_2017_2025.csv')  # Replace with your file path\n",
        "\n",
        "# Convert timestamp columns automatically\n",
        "df['Open Time'] = pd.to_datetime(df['Open Time'], errors='coerce')\n",
        "df['Close Time'] = pd.to_datetime(df['Close Time'], errors='coerce')\n",
        "\n",
        "# Drop rows where timestamp conversion failed\n",
        "df.dropna(subset=['Open Time','Close Time'], inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 3. Keep only relevant columns\n",
        "# -------------------------\n",
        "df = df[['Open Time','Open','High','Low','Close','Volume','Quote Volume','Trades']]\n",
        "\n",
        "# -------------------------\n",
        "# 4. Handle missing values\n",
        "# -------------------------\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# 5. Feature Engineering\n",
        "# -------------------------\n",
        "df['HL_PCT'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
        "df['PCT_change'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
        "\n",
        "# -------------------------\n",
        "# 6. Scaling / Normalization\n",
        "# -------------------------\n",
        "features = ['Open','High','Low','Close','Volume','Quote Volume','Trades','HL_PCT','PCT_change']\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
        "scaled_df['Open Time'] = df['Open Time'].values\n",
        "\n",
        "# -------------------------\n",
        "# 7. Save entire cleaned & scaled dataset\n",
        "# -------------------------\n",
        "scaled_df.to_csv('BTC_cleaned_scaled.csv', index=False)\n",
        "print(\"Entire BTC cleaned & scaled dataset saved successfully as 'BTC_cleaned_scaled.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bkPWEGXD-2E",
        "outputId": "b587047d-fb1e-4e70-c402-5124738e8b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entire BTC cleaned & scaled dataset saved successfully as 'BTC_cleaned_scaled.csv'\n"
          ]
        }
      ]
    }
  ]
}